# Efficient SAMWISE

Efficient SAMWISE is a practical repo to run, compare, and extend SAMWISE for text-driven video segmentation with speed-focused variants.

- Baseline: SAMWISE (CVPR 2025) with Hiera backbone (highest accuracy)
- Fast: SAMWISE with a lighter memory-attention path inspired by EdgeTAM (better latency, similar quality)
- Optional: Explore alternative backbones (e.g., RepViT via TIMM) for research; retraining recommended to preserve accuracy

## Why this repo
- Easy, single place to run the official SAMWISE demo on your own data
- Compare default vs faster settings with clear commands
- A clean base to iterate on efficiency improvements

## Quick start
1) Create environment
```bash
conda create -n samwise python=3.10 -y
conda activate samwise
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
pip install -r SAMWISE/requirements.txt
```

2) Optional speed libraries
```bash
pip install xformers "flash-attn>=2.5.6" --no-build-isolation
```

3) Download SAMWISE checkpoints (first run auto-downloads SAM2 weights and RoBERTa)
```bash
cd SAMWISE
pip install gdown
# Video-level weights (default for .mp4)
gdown --fuzzy "https://drive.google.com/file/d/1Molt2up2bP41ekeczXWQU-LWTskKJOV2/view?usp=sharing" -O pretrain/final_model_mevis.pth
# (Optional) Image-level weights
gdown --fuzzy "https://drive.google.com/file/d/1gRGzARDjIisZ3PnCW77Y9TMM_SbV8aaa/view?usp=drive_link" -O pretrain/pretrained_model.pth
```

## Run demos
From `SAMWISE/`:

- Baseline (highest accuracy)
```bash
python inference_demo.py \
  --input_path assets/example_video.mp4 \
  --text_prompts "the horse jumping"
```

- Fast (EdgeTAM-style memory attention)
```bash
python inference_demo.py \
  --input_path assets/example_video.mp4 \
  --text_prompts "the horse jumping" \
  --sam2_config sam2_configs/edgetam_hiera.yaml
```

Outputs are written to `SAMWISE/demo_output/<prompt>/` and `<prompt>.mp4`.

## Notes on speed/accuracy
- Fast mode reduces memory-attention depth and uses rotary-encoded cross-attention; expect better latency with similar quality.
- Further speed-ups (lower image size, fewer memory frames) will trade accuracyâ€”tune cautiously.
- Alternative backbones (e.g., RepViT via TIMM) need fine-tuning to maintain accuracy; without training, masks may degrade.

## Acknowledgments
- SAMWISE: "SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation" (CVPR 2025)
- EdgeTAM: Efficient temporal attention/memory ideas that inspired our fast configuration

This repository repackages and orchestrates the official SAMWISE code for ease of comparison and deployment while crediting the original authors.
